{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28c23db",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Environment (in Google Colab)\n",
    "!pip install transformers datasets seqeval accelerate -q\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf53384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load CoNLL formatted data and preprocess\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Davlan/afroxlmr-base\"  # or 'Davlan/bert-tiny-amharic'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Load your own CoNLL file manually\n",
    "from datasets import Dataset\n",
    "\n",
    "def read_conll(path):\n",
    "    sentences, labels = [], []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                splits = line.strip().split()\n",
    "                tokens.append(splits[0])\n",
    "                tags.append(splits[1])\n",
    "    return pd.DataFrame({\"tokens\": sentences, \"ner_tags\": labels})\n",
    "\n",
    "train_df = read_conll(\"labeled_conll.txt\")\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "label_list = list(set(tag for row in train_df[\"ner_tags\"] for tag in row))\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "def encode(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    labels = []\n",
    "    prev_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            labels.append(-100)\n",
    "        elif word_id != prev_word:\n",
    "            labels.append(label2id[example[\"ner_tags\"][word_id]])\n",
    "        else:\n",
    "            labels.append(label2id[example[\"ner_tags\"][word_id]])\n",
    "        prev_word = word_id\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "encoded_dataset = dataset.map(encode, batched=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1dcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fine-tune the model\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./ner_model\")\n",
    "tokenizer.save_pretrained(\"./ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Interpretability (SHAP + LIME for Transformers)\n",
    "!pip install shap lime -q\n",
    "\n",
    "import shap\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=\"./ner_model\", tokenizer=\"./ner_model\", grouped_entities=True)\n",
    "explainer = shap.Explainer(ner_pipeline)\n",
    "shap_values = explainer([\"ዋጋ 1000 ብር ቦሌ ሞል ላይ LCD Tablet አለ\"])\n",
    "shap.plots.text(shap_values[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
